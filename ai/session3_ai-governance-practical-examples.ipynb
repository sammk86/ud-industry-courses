{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKwdzyVmyQgfYGmo"
      },
      "source": [
        "# AI Governance: Practical Examples\n\n**Session 3: Responsible AI Implementation**\n\nThis notebook demonstrates three core concepts in AI governance:\n1. **Fairness Evaluation** - Detecting and measuring bias in ML models\n2. **Bias Mitigation** - Techniques to reduce unfairness\n3. **Model Monitoring** - Tracking governance metrics in production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFYtZ11r7Gl0lDrv"
      },
      "source": [
        "## Introduction\n\nAI governance ensures ML systems are fair, transparent, accountable, and safe. In this notebook, you'll build practical skills for:\n\n- Evaluating fairness across demographic groups\n- Detecting bias in model predictions\n- Implementing monitoring dashboards\n\nWe'll use a **credit scoring scenario** as our working example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzkpA7nsTs68X7fs"
      },
      "source": [
        "---\n## Example 1: Fairness Evaluation\n\n**Objective**: Measure if a credit scoring model treats different demographic groups fairly.\n\nWe'll evaluate three fairness metrics:\n- **Demographic Parity**: Equal approval rates across groups\n- **Equalized Odds**: Equal TPR and FPR across groups  \n- **Predictive Parity**: Equal precision across groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnX7QkxgVuQvoz1S"
      },
      "source": [
        "# Install required library\n!pip install -q scikit-learn pandas numpy matplotlib\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Simulate credit scoring data\nnp.random.seed(42)\nn_samples = 1000\n\n# Create synthetic dataset\ndata = {\n    'income': np.random.normal(50000, 20000, n_samples),\n    'credit_history': np.random.uniform(300, 850, n_samples),\n    'age': np.random.randint(22, 70, n_samples),\n    'gender': np.random.choice(['M', 'F'], n_samples),\n    'approved': np.random.choice([0, 1], n_samples, p=[0.4, 0.6])\n}\n\n# Introduce bias: males get higher approval at same credit score\nfor i in range(n_samples):\n    if data['gender'][i] == 'M' and data['credit_history'][i] > 600:\n        data['approved'][i] = 1 if np.random.random() > 0.2 else 0\n    elif data['gender'][i] == 'F' and data['credit_history'][i] > 600:\n        data['approved'][i] = 1 if np.random.random() > 0.4 else 0\n\ndf = pd.DataFrame(data)\nprint(\"Dataset shape:\", df.shape)\nprint(\"\\nFirst few rows:\")\nprint(df.head())\nprint(\"\\nApproval rate by gender:\")\nprint(df.groupby('gender')['approved'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrbjjeIkLYceMGmt"
      },
      "source": [
        "def evaluate_fairness(df, sensitive_attr='gender'):\n    \"\"\"\n    Calculate fairness metrics across demographic groups.\n    \"\"\"\n    results = {}\n    \n    # 1. DEMOGRAPHIC PARITY\n    # Equal approval rates across groups\n    approval_rates = df.groupby(sensitive_attr)['approved'].mean()\n    results['approval_rates'] = approval_rates\n    results['demographic_parity_diff'] = approval_rates.max() - approval_rates.min()\n    \n    # 2. EQUALIZED ODDS (simplified: just TPR for this demo)\n    for group in df[sensitive_attr].unique():\n        group_data = df[df[sensitive_attr] == group]\n        # Simulate ground truth (in real scenario, you'd have actual labels)\n        # For demo: assume approval correlates with credit history\n        y_true = (group_data['credit_history'] > 650).astype(int)\n        y_pred = group_data['approved']\n        \n        if len(y_true) > 0 and y_true.sum() > 0:\n            tpr = ((y_pred == 1) & (y_true == 1)).sum() / y_true.sum()\n            results[f'TPR_{group}'] = tpr\n    \n    return results\n\n# Evaluate fairness\nfairness_results = evaluate_fairness(df)\n\nprint(\"=\"*50)\nprint(\"FAIRNESS EVALUATION RESULTS\")\nprint(\"=\"*50)\nfor metric, value in fairness_results.items():\n    if isinstance(value, (int, float)):\n        print(f\"{metric}: {value:.3f}\")\n    else:\n        print(f\"{metric}:\")\n        print(value)\n\n# Visualize approval rates\nplt.figure(figsize=(8, 5))\nfairness_results['approval_rates'].plot(kind='bar', color=['#FF6B6B', '#4ECDC4'])\nplt.title('Approval Rates by Gender', fontsize=14, fontweight='bold')\nplt.ylabel('Approval Rate')\nplt.xlabel('Gender')\nplt.xticks(rotation=0)\nplt.axhline(y=0.6, color='gray', linestyle='--', label='Overall Rate')\nplt.legend()\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Interpretation\nprint(\"\\n\ud83d\udcca INTERPRETATION:\")\nparity_diff = fairness_results['demographic_parity_diff']\nif parity_diff < 0.1:\n    print(f\"\u2705 Demographic parity difference ({parity_diff:.3f}) is acceptable (< 0.1)\")\nelse:\n    print(f\"\u26a0\ufe0f Demographic parity difference ({parity_diff:.3f}) exceeds threshold!\")\n    print(\"   This indicates potential bias in approval rates.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGZ5gMHMCi3He2np"
      },
      "source": [
        "---\n## Example 2: Bias Mitigation\n\n**Objective**: Apply a simple bias mitigation technique - threshold adjustment.\n\nWhen we detect unfairness, we can:\n1. **Reweighting**: Adjust training data weights\n2. **Threshold Optimization**: Use different decision thresholds per group\n3. **Adversarial Debiasing**: Train model to be fair\n\nWe'll demonstrate **threshold adjustment** as it's simple yet effective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo2EzoD3SONNVAfW"
      },
      "source": [
        "# Bias Mitigation: Threshold Adjustment\ndef adjust_thresholds(df, sensitive_attr='gender', target_parity=0.05):\n    \"\"\"\n    Adjust approval thresholds to achieve demographic parity.\n    \"\"\"\n    # Calculate current approval rates\n    overall_rate = df['approved'].mean()\n    \n    print(f\"Overall approval rate: {overall_rate:.3f}\")\n    print(\"\\nCurrent approval rates by group:\")\n    for group in df[sensitive_attr].unique():\n        group_rate = df[df[sensitive_attr] == group]['approved'].mean()\n        print(f\"  {group}: {group_rate:.3f}\")\n    \n    # Simulate threshold adjustment\n    # In practice, you'd adjust based on probability scores\n    df_adjusted = df.copy()\n    \n    # For the disadvantaged group, boost approvals\n    disadvantaged = 'F'  # Based on our analysis\n    boost_factor = 1.2  # 20% boost\n    \n    for idx in df_adjusted[df_adjusted[sensitive_attr] == disadvantaged].index:\n        if np.random.random() < 0.15:  # Adjust 15% of cases\n            df_adjusted.loc[idx, 'approved_adjusted'] = 1\n        else:\n            df_adjusted.loc[idx, 'approved_adjusted'] = df_adjusted.loc[idx, 'approved']\n    \n    # Keep original approvals for other groups\n    for idx in df_adjusted[df_adjusted[sensitive_attr] != disadvantaged].index:\n        df_adjusted.loc[idx, 'approved_adjusted'] = df_adjusted.loc[idx, 'approved']\n    \n    return df_adjusted\n\n# Apply mitigation\ndf_mitigated = adjust_thresholds(df)\n\n# Re-evaluate fairness\nprint(\"\\n\" + \"=\"*50)\nprint(\"AFTER BIAS MITIGATION\")\nprint(\"=\"*50)\n\napproval_rates_after = df_mitigated.groupby('gender')['approved_adjusted'].mean()\nparity_diff_after = approval_rates_after.max() - approval_rates_after.min()\n\nprint(f\"\\nNew approval rates:\")\nfor group, rate in approval_rates_after.items():\n    print(f\"  {group}: {rate:.3f}\")\nprint(f\"\\nDemographic parity difference: {parity_diff_after:.3f}\")\n\n# Visualize before/after\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Before\nfairness_results['approval_rates'].plot(kind='bar', ax=ax1, color=['#FF6B6B', '#4ECDC4'])\nax1.set_title('Before Mitigation', fontsize=12, fontweight='bold')\nax1.set_ylabel('Approval Rate')\nax1.set_xlabel('Gender')\nax1.set_ylim(0, 1)\nax1.grid(axis='y', alpha=0.3)\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\n\n# After\napproval_rates_after.plot(kind='bar', ax=ax2, color=['#95E1D3', '#FFB6B9'])\nax2.set_title('After Mitigation', fontsize=12, fontweight='bold')\nax2.set_ylabel('Approval Rate')\nax2.set_xlabel('Gender')\nax2.set_ylim(0, 1)\nax2.grid(axis='y', alpha=0.3)\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n\u2705 Bias reduced from {fairness_results['demographic_parity_diff']:.3f} to {parity_diff_after:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pJkpWbLRP8tJRxi"
      },
      "source": [
        "---\n## Example 3: Model Monitoring Dashboard\n\n**Objective**: Build a simple monitoring system to track governance metrics over time.\n\nIn production, you must continuously monitor:\n- Performance metrics (accuracy, precision)\n- Fairness metrics (demographic parity)\n- Data drift (distribution changes)\n- Security alerts (adversarial attacks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WOfZfaL7VGRxNkO"
      },
      "source": [
        "# Simulate production monitoring data\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Simulate 30 days of monitoring data\ndates = [datetime.now() - timedelta(days=x) for x in range(30, 0, -1)]\nmonitoring_data = []\n\nfor date in dates:\n    # Simulate metrics with some drift over time\n    drift_factor = (30 - (datetime.now() - date).days) / 30\n    \n    monitoring_data.append({\n        'date': date,\n        'accuracy': 0.85 + np.random.normal(0, 0.02) - (0.01 * (1 - drift_factor)),\n        'demographic_parity_diff': 0.08 + np.random.normal(0, 0.01) + (0.05 * (1 - drift_factor)),\n        'prediction_latency_ms': 45 + np.random.normal(0, 5) + (10 * (1 - drift_factor)),\n        'throughput_qps': 1000 + np.random.normal(0, 50),\n        'drift_score': 0.15 + (0.3 * (1 - drift_factor)) + np.random.normal(0, 0.02)\n    })\n\nmonitor_df = pd.DataFrame(monitoring_data)\n\nprint(\"Monitoring Data (Last 5 days):\")\nprint(monitor_df.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvUjNX8HnW9GcOQW"
      },
      "source": [
        "# Create monitoring dashboard\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\nfig.suptitle('AI Governance Monitoring Dashboard', fontsize=16, fontweight='bold')\n\n# 1. Accuracy over time\naxes[0, 0].plot(monitor_df['date'], monitor_df['accuracy'], \n                marker='o', linewidth=2, color='#4ECDC4')\naxes[0, 0].axhline(y=0.85, color='green', linestyle='--', label='Baseline')\naxes[0, 0].axhline(y=0.80, color='red', linestyle='--', label='Alert Threshold')\naxes[0, 0].set_title('Model Accuracy', fontweight='bold')\naxes[0, 0].set_ylabel('Accuracy')\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# 2. Fairness metrics\naxes[0, 1].plot(monitor_df['date'], monitor_df['demographic_parity_diff'], \n                marker='s', linewidth=2, color='#FF6B6B')\naxes[0, 1].axhline(y=0.1, color='orange', linestyle='--', label='Threshold')\naxes[0, 1].axhline(y=0.2, color='red', linestyle='--', label='Critical')\naxes[0, 1].set_title('Fairness: Demographic Parity Difference', fontweight='bold')\naxes[0, 1].set_ylabel('Parity Difference')\naxes[0, 1].legend()\naxes[0, 1].grid(alpha=0.3)\naxes[0, 1].tick_params(axis='x', rotation=45)\n\n# 3. Latency\naxes[1, 0].plot(monitor_df['date'], monitor_df['prediction_latency_ms'], \n                marker='^', linewidth=2, color='#95E1D3')\naxes[1, 0].axhline(y=50, color='orange', linestyle='--', label='SLA Limit')\naxes[1, 0].set_title('Prediction Latency', fontweight='bold')\naxes[1, 0].set_ylabel('Latency (ms)')\naxes[1, 0].set_xlabel('Date')\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\naxes[1, 0].tick_params(axis='x', rotation=45)\n\n# 4. Drift detection\naxes[1, 1].plot(monitor_df['date'], monitor_df['drift_score'], \n                marker='D', linewidth=2, color='#FFB6B9')\naxes[1, 1].axhline(y=0.2, color='orange', linestyle='--', label='Warning')\naxes[1, 1].axhline(y=0.4, color='red', linestyle='--', label='Alert')\naxes[1, 1].set_title('Data Drift Score', fontweight='bold')\naxes[1, 1].set_ylabel('Drift Score (PSI)')\naxes[1, 1].set_xlabel('Date')\naxes[1, 1].legend()\naxes[1, 1].grid(alpha=0.3)\naxes[1, 1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# Alert system\nprint(\"\\n\" + \"=\"*50)\nprint(\"ALERT SUMMARY\")\nprint(\"=\"*50)\n\nalerts = []\nif monitor_df['accuracy'].iloc[-1] < 0.80:\n    alerts.append(\"\ud83d\udea8 CRITICAL: Accuracy below threshold!\")\nif monitor_df['demographic_parity_diff'].iloc[-1] > 0.2:\n    alerts.append(\"\ufffd\ufffd CRITICAL: Fairness violation detected!\")\nif monitor_df['drift_score'].iloc[-1] > 0.4:\n    alerts.append(\"\u26a0\ufe0f WARNING: Significant data drift detected!\")\nif monitor_df['prediction_latency_ms'].iloc[-1] > 50:\n    alerts.append(\"\u26a0\ufe0f WARNING: Latency exceeds SLA!\")\n\nif alerts:\n    for alert in alerts:\n        print(alert)\nelse:\n    print(\"\u2705 All metrics within acceptable ranges\")\n\n# Recommendations\nprint(\"\\n\ud83d\udccb RECOMMENDATIONS:\")\nif monitor_df['drift_score'].iloc[-1] > 0.3:\n    print(\"\u2022 Consider retraining model on recent data\")\nif monitor_df['demographic_parity_diff'].iloc[-1] > 0.15:\n    print(\"\u2022 Review and adjust fairness mitigation strategies\")\nif monitor_df['accuracy'].iloc[-1] < monitor_df['accuracy'].iloc[0] - 0.05:\n    print(\"\u2022 Investigate performance degradation root cause\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHoEVWo9LKfgj00"
      },
      "source": [
        "---\n## Summary & Key Takeaways\n\nIn this notebook, you learned to:\n\n\u2705 **Evaluate fairness** using demographic parity, equalized odds, and predictive parity metrics  \n\u2705 **Detect bias** in ML models across demographic groups  \n\u2705 **Mitigate unfairness** using threshold adjustment techniques  \n\u2705 **Monitor governance metrics** in production systems with dashboards  \n\n### Next Steps\n\n1. **Practice**: Apply these techniques to your own datasets\n2. **Explore**: Try other fairness libraries (AI Fairness 360, Fairlearn)\n3. **Extend**: Build automated alerting systems for governance violations\n4. **Study**: Review the full architecture documentation for deeper concepts\n\n### Additional Resources\n\n- [AI Fairness 360](https://aif360.mybluemix.net/) - IBM's fairness toolkit\n- [Fairlearn](https://fairlearn.org/) - Microsoft's fairness library\n- [NIST AI RMF](https://www.nist.gov/itl/ai-risk-management-framework) - Risk management framework\n- [EU AI Act](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai) - Regulatory guidance\n\n---\n**Session 3: AI Governance** | Built with \u2764\ufe0f for responsible AI"
      ]
    }
  ]
}