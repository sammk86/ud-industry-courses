{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"id": "a1b2c3d4e5f6g7h8"},
      "source": ["# Unsupervised Learning & Clustering - Hands-On Lab\n\nüìö **Objective**: Learn unsupervised learning through 3 practical examples\n\nBy completing this notebook, you'll understand how clustering algorithms work in practice and when to use each one."]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "i9j0k1l2m3n4o5p6"},
      "source": ["## Introduction\n\nUnsupervised learning discovers hidden patterns in unlabeled data. This notebook demonstrates **three core clustering approaches**:\n\n1. **K-means** - Fast partitional clustering for customer segmentation\n2. **DBSCAN** - Density-based clustering for anomaly detection\n3. **Comparison** - Evaluating clustering quality with metrics\n\nEach example is self-contained and produces visual output to demonstrate the concepts."]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "q7r8s9t0u1v2w3x4"},
      "source": ["---\n## Example 1: K-means for Customer Segmentation\n\n**Goal**: Group customers by purchasing behavior (Recency, Frequency, Monetary)\n\n**Key Concepts**:\n- Feature scaling before clustering\n- Elbow method to find optimal K\n- Silhouette score for evaluation"]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "y5z6a7b8c9d0e1f2"},
      "source": ["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n\n# Generate synthetic customer data\nnp.random.seed(42)\nn_customers = 300\n\ndata = pd.DataFrame({\n    'recency': np.random.randint(1, 365, n_customers),      # Days since last purchase\n    'frequency': np.random.randint(1, 50, n_customers),     # Number of purchases  \n    'monetary': np.random.randint(10, 1000, n_customers)    # Average order value\n})\n\nprint(\"üìä Customer Data Sample:\")\nprint(data.head())\nprint(f\"\\nShape: {data.shape}\")\nprint(f\"\\nStatistics:\\n{data.describe()}\")"],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "g3h4i5j6k7l8m9n0"},
      "source": ["### Step 1: Feature Scaling\n\n‚ö†Ô∏è **Critical**: K-means uses Euclidean distance, so features must be on the same scale!"]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "o1p2q3r4s5t6u7v8"},
      "source": ["# Standardize features (mean=0, std=1)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(data)\n\nprint(\"‚úÖ Scaling verification:\")\nprint(f\"Mean: {X_scaled.mean(axis=0).round(2)}\")  # Should be ~[0, 0, 0]\nprint(f\"Std:  {X_scaled.std(axis=0).round(2)}\")   # Should be ~[1, 1, 1]"],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "w9x0y1z2a3b4c5d6"},
      "source": ["### Step 2: Find Optimal K (Elbow Method)\n\nWe'll try K from 2 to 10 and plot **inertia** (within-cluster sum of squares) and **silhouette scores**."]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "e7f8g9h0i1j2k3l4"},
      "source": ["# Try different K values\nK_range = range(2, 11)\ninertias = []\nsilhouette_scores = []\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nax1.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\nax1.set_xlabel('Number of Clusters (K)', fontsize=12)\nax1.set_ylabel('Inertia', fontsize=12)\nax1.set_title('Elbow Method', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\n\nax2.plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\nax2.set_xlabel('Number of Clusters (K)', fontsize=12)\nax2.set_ylabel('Silhouette Score', fontsize=12)\nax2.set_title('Silhouette Analysis', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüìà Best K by silhouette: {K_range[np.argmax(silhouette_scores)]}\")"],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "m5n6o7p8q9r0s1t2"},
      "source": ["### Step 3: Apply K-means with Optimal K\n\nBased on the elbow plot, let's choose **K=4** (look for the \"elbow\" where inertia starts decreasing slowly)."]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "u3v4w5x6y7z8a9b0"},
      "source": ["# Fit final model\noptimal_k = 4\nkmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nclusters = kmeans_final.fit_predict(X_scaled)\n\n# Add clusters to original data\ndata['cluster'] = clusters\n\n# Evaluate\nfinal_silhouette = silhouette_score(X_scaled, clusters)\nprint(f\"‚úÖ Silhouette Score (K={optimal_k}): {final_silhouette:.3f}\")\nprint(f\"   Range: [-1, 1], Higher is better\\n\")\n\n# Analyze cluster profiles\ncluster_summary = data.groupby('cluster').mean()\nprint(\"üìä Cluster Profiles (Original Scale):\")\nprint(cluster_summary.round(0))\nprint(\"\\nüí° Interpretation:\")\nprint(\"  - Look for patterns in recency, frequency, and monetary values\")\nprint(\"  - High recency = Recent buyers\")\nprint(\"  - High frequency = Loyal customers\")\nprint(\"  - High monetary = Big spenders\")"],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "c1d2e3f4g5h6i7j8"},
      "source": ["### Step 4: Visualize Customer Segments"]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "k9l0m1n2o3p4q5r6"},
      "source": ["# 2D visualization (Frequency vs Monetary)\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(data['frequency'], data['monetary'], \n                     c=data['cluster'], cmap='viridis', \n                     s=80, alpha=0.6, edgecolors='black', linewidth=0.5)\n\nplt.xlabel('Purchase Frequency', fontsize=12)\nplt.ylabel('Average Order Value ($)', fontsize=12)\nplt.title(f'Customer Segments (K-means, K={optimal_k})', fontsize=14, fontweight='bold')\nplt.colorbar(scatter, label='Cluster ID')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úÖ Customers successfully segmented! Marketing can now target each group differently.\")"],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "s7t8u9v0w1x2y3z4"},
      "source": ["---\n## Example 2: DBSCAN for Anomaly Detection\n\n**Goal**: Detect unusual network traffic patterns (anomalies)\n\n**Key Concepts**:\n- Density-based clustering\n- Œµ (epsilon) and MinPts parameters\n- Automatic outlier detection"]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "a5b6c7d8e9f0g1h2"},
      "source": ["from sklearn.cluster import DBSCAN\n\n# Generate data: normal traffic + anomalies\nnp.random.seed(42)\n\n# Two normal traffic clusters\nnormal1 = np.random.randn(100, 2) * 0.5 + [2, 2]\nnormal2 = np.random.randn(100, 2) * 0.5 + [8, 8]\nnormal = np.vstack([normal1, normal2])\n\n# Scattered anomalies\nanomalies = np.random.uniform(0, 10, (20, 2))\n\n# Combine\nX = np.vstack([normal, anomalies])\n\nprint(f\"üì° Network Traffic Data:\")\nprint(f\"  Total points: {len(X)}\")\nprint(f\"  Normal patterns: 200\")\nprint(f\"  True anomalies: 20\")"],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "i3j4k5l6m7n8o9p0"},
      "source": ["### Apply DBSCAN\n\n**Parameters**:\n- `eps` (Œµ): Maximum distance between neighbors\n- `min_samples`: Minimum points to form a dense region"]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "q1r2s3t4u5v6w7x8"},
      "source": ["# Apply DBSCAN\nepsilon = 0.8\nmin_samples = 4\n\ndbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\nlabels = dbscan.fit_predict(X)\n\n# Analyze results\nn_clusters = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise = list(labels).count(-1)\n\nprint(f\"üîç DBSCAN Results:\")\nprint(f\"  Œµ (epsilon): {epsilon}\")\nprint(f\"  MinPts: {min_samples}\")\nprint(f\"  Clusters found: {n_clusters}\")\nprint(f\"  Noise points (anomalies): {n_noise}\")\nprint(f\"  Detection rate: {n_noise/20*100:.0f}% of true anomalies\")"],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "y9z0a1b2c3d4e5f6"},
      "source": ["### Visualize Clusters and Anomalies\n\nNoise points (label=-1) are marked with **red X** symbols."]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "g7h8i9j0k1l2m3n4"},
      "source": ["plt.figure(figsize=(10, 7))\n\n# Plot normal clusters and anomalies\nfor label in set(labels):\n    if label == -1:\n        # Anomalies (noise)\n        mask = (labels == label)\n        plt.scatter(X[mask, 0], X[mask, 1], c='red', marker='x', \n                   s=150, linewidths=3, label='Anomaly', zorder=3)\n    else:\n        # Normal clusters\n        mask = (labels == label)\n        plt.scatter(X[mask, 0], X[mask, 1], s=60, alpha=0.7, \n                   label=f'Cluster {label}', edgecolors='black', linewidth=0.5)\n\nplt.xlabel('Feature 1 (Packet Size)', fontsize=12)\nplt.ylabel('Feature 2 (Duration)', fontsize=12)\nplt.title(f'DBSCAN: Anomaly Detection (Œµ={epsilon}, MinPts={min_samples})', \n         fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n‚úÖ Anomaly detection complete! Red X marks indicate suspicious traffic.\")"],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "o5p6q7r8s9t0u1v2"},
      "source": ["---\n## Example 3: Comparing Clustering Algorithms\n\n**Goal**: Compare K-means, DBSCAN, and Hierarchical clustering on the same dataset\n\n**Metrics**:\n- Silhouette Score (higher = better)\n- Execution time"]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "w3x4y5z6a7b8c9d0"},
      "source": ["from sklearn.cluster import AgglomerativeClustering\nimport time\n\n# Use the scaled customer data\nX_comparison = X_scaled\n\n# Store results\nresults = {}\n\n# --- K-means ---\nstart = time.time()\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\nlabels_kmeans = kmeans.fit_predict(X_comparison)\ntime_kmeans = time.time() - start\n\nresults['K-means'] = {\n    'Silhouette': silhouette_score(X_comparison, labels_kmeans),\n    'Time (s)': time_kmeans,\n    'Clusters': 4\n}\n\n# --- DBSCAN ---\nstart = time.time()\ndbscan_comp = DBSCAN(eps=0.5, min_samples=5)\nlabels_dbscan_comp = dbscan_comp.fit_predict(X_comparison)\ntime_dbscan = time.time() - start\n\nn_clusters_dbscan = len(set(labels_dbscan_comp)) - (1 if -1 in labels_dbscan_comp else 0)\nif n_clusters_dbscan > 1:\n    sil_db = silhouette_score(X_comparison, labels_dbscan_comp)\nelse:\n    sil_db = 0.0\n\nresults['DBSCAN'] = {\n    'Silhouette': sil_db,\n    'Time (s)': time_dbscan,\n    'Clusters': n_clusters_dbscan\n}\n\n# --- Hierarchical ---\nstart = time.time()\nhierarchical = AgglomerativeClustering(n_clusters=4, linkage='ward')\nlabels_hierarchical = hierarchical.fit_predict(X_comparison)\ntime_hierarchical = time.time() - start\n\nresults['Hierarchical'] = {\n    'Silhouette': silhouette_score(X_comparison, labels_hierarchical),\n    'Time (s)': time_hierarchical,\n    'Clusters': 4\n}\n\n# Display comparison\ncomparison_df = pd.DataFrame(results).T\nprint(\"üìä Algorithm Comparison:\")\nprint(comparison_df.round(4))\nprint(\"\\nüí° Key Insights:\")\nprint(\"  - Higher Silhouette = Better clustering quality\")\nprint(\"  - K-means: Fastest, good for spherical clusters\")\nprint(\"  - DBSCAN: Finds arbitrary shapes, identifies outliers\")\nprint(\"  - Hierarchical: No K needed upfront, slower for large datasets\")"],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "e1f2g3h4i5j6k7l8"},
      "source": ["---\n## üìö Summary & Key Takeaways\n\n### What You Learned:\n\n1. **K-means Clustering**\n   - Requires K (number of clusters) upfront\n   - Use elbow method & silhouette score to find optimal K\n   - **Always scale features** before applying!\n   - Best for: Spherical, similarly-sized clusters\n\n2. **DBSCAN Clustering**\n   - No K required - discovers clusters automatically\n   - Identifies **outliers** as noise points\n   - Parameters: Œµ (neighborhood radius) and MinPts\n   - Best for: Arbitrary shapes, anomaly detection\n\n3. **Evaluation Metrics**\n   - **Silhouette Score**: [-1, 1], higher is better\n   - Compare multiple algorithms on same data\n   - No single \"correct\" clustering - depends on goal!\n\n### üöÄ Next Steps:\n- Try Gaussian Mixture Models (GMM) for soft clustering\n- Explore hierarchical clustering dendrograms\n- Apply to your own datasets\n- Learn dimensionality reduction (PCA, t-SNE) for high-dimensional data\n\n### ‚ö†Ô∏è Remember:\n- Clustering is **exploratory** - validate results with domain knowledge\n- Different algorithms reveal different patterns  \n- Preprocessing (scaling, handling outliers) is critical!"]
    }
  ]
}
